\documentclass[8pt]{article}

\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{tabularx}
\usepackage{mathabx}
\usepackage{enumitem}
\usepackage{caption}
\usepackage{multicol}
\usepackage{sectsty}
\usepackage{extsizes}
\usepackage{fancyhdr}
\usepackage{enumerate}
\usepackage[margin=1.5cm]{geometry}
\usepackage{multirow}

\pagestyle{fancyplain}

% Try very hard not to break relational and binary operators (equations)
\relpenalty=9999
\binoppenalty=9999

\newcommand{\dd}[1]{\mathrm{d}{#1}}
\newcommand{\ddt}[1]{\frac{\dd{}}{\dd{#1}}}
\newcommand{\dddt}[1]{\frac{\dd{}^2}{\dd{#1}^2}}

\begin{document}

\lhead{Hershal Bhave \quad\tiny{hb6279}}
\rhead{EE360C Fall 2014 Exam 1 Cheat Sheet (Pedro Santacruz)}

\begin{multicols}{2}
  \begin{description}
  \item[Directed Graph] $G$ is a pair $(V,E)$ where $V$ is a finite
    set of vertices and $E$, the edges is is a subset of $V \times V$.
  \item[Undirected Graph] $G$ is a pair $(V,E)$ where $V$ is a finite
    set of vertices and $E$, the edges is is a set of unordered
    pairs of edges $\{u,v\}$, where $u \not= v$.
  \item[Incidency] If $(u,v)$ is an edge in digraph $G$, then $(u,v)$
    is incident from $u$ (leaving) and incident to $v$ (entering). $v$
    is said to be adjacent to $u$, not necessarily symmetric for
    digraphs.
  \item[Degree] Undirected graph: number of edges incident to
    (entering) the vertex. Digraph: out-degree of a vertex the number
    of edges leaving it; in-degree number of edges entering it.
  \item[Paths] Length of a path is number of edges, $v$ is reachable
    from $u$ if here is a path from $v$ to $u$. A path is a subpath of
    itself. A path is simple if all vertices are distinct. Digraph
    paths form cycles if $v_0 = v_k$, $k\ge 1$ in the path; $k\ge3$
    for undirected graphs.
  \item[Connected] Undirected is connected if each pair of vertices is
    connected by a path (every pair of vertices are ``reachable'').
  \item[Strongly Connected] Directed: if every two vertices are
    reachable from one another (every pair of vertices are ``mutually
    reachable''). A digraph is strongly connected if it has exactly
    one strongly connected component.
  \item[Isomorphic] $G=(V,E)$ is isomorphic to $G'=(V',E')$ is there
    is a 1-1 onto function $f:V\rightarrow V'$ st $(u,v) \in E$ iff
    $(f(u),f(v)) \in E'$.
  \item[Proof by Induction] \hfill
    \begin{enumerate}
    \item Base Case: Show that $P(k_0)$ is true
    \item Inductive Step: Assume $P(n)$ is true. Show that $P(n+1)$ is
      true. Morph part of the $P(n+1)$ equation into $P(n)$.
    \end{enumerate}
  \item[Divisibility] $d$ divides $n$ ($n$ divisible by $d$): $d|n \Leftrightarrow
    \exists k \in \mathbb{Z}\text{ st } n = dk$.
  \item[Complete] Undirected, every pair of vertices is adjacent
  \item[Bipartite Graph] Undirected, vertices can be partitioned into
    $V_1$, $V_2$ st every edge is in the form $(x,y)$ where $x \in
    V_1$ and $y \in V_2$.
  \item[Forest] An acyclic undirected graph
  \item[Tree] A connected, acyclic undirected graph
  \item[DAG] Directed Acyclic Graph
  \item[Multigraph] Like undirected, but can have multiple edges
    between vertices and self-loops
  \item[Hypergraph] Like undirected, each hyperedge can connect to an
    arbitrary number of vertices
  \item[Properties of Free Trees] $G=(V,E)$ is an undirected graph
    \begin{itemize}
    \item Any two vertices are connected by a unique simple path
    \item If any edge is removed from $E$, the graph is not connected
    \item If any edge is added to $E$, then the graph contains a cycle
    \item $G$ is connected, acyclic, and $|E| = |V|-1$
    \end{itemize}
  \item[Rooted Tree] A free tree in which one vertex is distinguished
    from the others; the distingushed vertex is the root ($r$), others
    are called nodes ($x$). For any $x$, there is a unique path from
    $r$ to $x$. Any node $y$ on a path from $r$ to $x$ is an ancestor
    of $x$. Every node is its own ancestor and descendant. A proper
    ancestor (descendant) is an ancestor (descendant) that is not the
    node itself. The subtree rooted at $x$ is the tree introduced by
    the descendants of $x$.
  \item[Asy. Upper Bounds] \hfill \\
    $\mathcal{O}(g(n))=\{f(n): \exists c,n_0>0 \text{ st } 0 \le f(n)
    \le cg(n) \quad\forall n \ge n_0\}$; $\Rightarrow
    \lim_{n\rightarrow\infty}\frac{f(n)}{g(n)} = 0 \text{ or } c$;
    $a \le b$
  \item[Asy. Lower Bounds] \hfill\\
    $\Omega(g(n))=\{f(n): \exists c,n_0>0 \text{ st } 0 \le cg(n) \le
    f(n) \quad\forall n \ge n_0\}$; $\Rightarrow
    \lim_{n\rightarrow\infty}\frac{f(n)}{g(n)} = \infty \text{ or }
    c$;
    $a \ge b$
  \item[Asy Tight Bounds] \hfill\\
    $\Theta(g(n))=\{f(n): \exists c_1,c_2,n_0>0 \text{ st } 0 \le
    c_1g(n) \le f(n) \le c_2g(n) \quad\forall n \ge n_0\}$;
    $\Rightarrow \lim_{n\rightarrow\infty}\frac{f(n)}{g(n)} = c$;
    $a = b$
  \item[Asy Non-Tight Bounds $o$ and $\omega$] Replace $\le$ with $<$
  \item[General Rules for Complexity] Increasing order:
    logs, polynomials, exponentials
  \item[Heap Principles] Heaps are loosly ordered in that each child
    in a min-heap is greater than its parent (that's it). The root is
    the min value. After every insertion the tree is rebalanced using
    \texttt{heapify\_up}.
  \item[Fix Heap After Insertion] \hfill
\begin{verbatim}
heapify_up(H,i) {
  if (i > 1) {
    let j = parent(i);
    if (key(H(i)) < key(H(j))) {
      swap(H, i, j);
      heapify_up(H,j);
}  }  }
\end{verbatim}
    Runs $\mathcal{O}(\log i)$.
  \item[Fix Heap After Deletion] \hfill
\begin{verbatim}
heapify_down(H,i) {
  let n=length(H);
  if (2*i > n) {
    return;
  } else if ((2*i) < n) {
    let left = 2*i;
    let right = 2*i+1;
    let j = min(key(H(left)),
                key(H(right)));
  } else if ((2*i) == n) {
    let j= 2*i;
  }
  if (key(H(j)) < key(H(i))) {
    swap(H, i, j)
    heapify_down(H, j);
}  }
\end{verbatim}
    Runs $\mathcal{O}(\log n)$.
  \item[Decision Trees] A Decision Tree is a ``full tree'' which
    represents the comparisons between elements performed by a sorting
    algorithm. Internal nodes are annotated $i:j$ for some $i$ and $j$
    in $1 \le i, j \le n$ for $n$ elements in the input
    sequence. Leaves are annotated by a permutation $\langle \pi(1),
    \pi(2), \ldots, \pi(n)\rangle$. Executing a sorting algorithm
    equates to tracing through the decision tree
  \item[Log Rules] 
    \begin{equation*}
      \begin{aligned}
        a &= b^{\log_b a} &
        \log_c(ab) &= \log_c(a) + \log_c(b) \\
        \log_b a &= \frac{\log_c a}{\log_c b} &
        \log_b(1/a) &= -\log_b a \\
        \log_b a &= \frac{1}{\log_a b} &
        a^{\log_b c} &= c^{log_b a} \\
        y = b^x &\Leftrightarrow x=\log_b y &
      \end{aligned}
    \end{equation*}
  \item[Lower Bound For Worst-Case] The worst case nuber of
    comparisons for a comparison sort algorithm is the height of its
    decision tree, usually $\Omega(n\log n)$: We must determine the
    height of a decision tree with $n!$ leaves. A binary tree of
    height $h$ has no more than $2^h$ leaves, therefore $n!\le2^h$, so
    $h\ge\log(n!) = \Omega(n\log n)$
  \item[Data Structure Stats] Priority Queues: $\mathcal{O}(\log n)$
    ins/del $\mathcal{O}(n\log n)$ sorting/find-min. Lists:
    $\mathcal{O}(1)$ ins/del, $\Omega(n)$ find-min. Sorted Array:
    $\Omega(n)$ ins/del, $\mathcal{O}(1)$ find-min.
  \end{description} % multicols
\end{multicols}
\end{document}